{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Hyperparameter Tuning using HyperDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598531914256
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core import Environment, ScriptRunConfig\n",
    "\n",
    "from azureml.train.hyperdrive.policy import NoTerminationPolicy\n",
    "from azureml.train.hyperdrive.sampling import BayesianParameterSampling\n",
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive.parameter_expressions import choice, uniform\n",
    "\n",
    "from azureml.widgets import RunDetails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598531917374
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "experiment_name = \"creditcard-experiment\"\n",
    "project_folder = './creditcard-hyperdrive-project'\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "run = experiment.start_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amlcompute_cluster_name = \"automl-cls\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
    "    print(\"Found existing cluster, use it.\")\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_D2_v3\", max_nodes=4)\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True, min_node_count=0, timeout_in_minutes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"creditcard-dataset\"\n",
    "description = \"Credit Card - Dealing from Imbalance Datasets from https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\"\n",
    "\n",
    "found = False\n",
    "if key in ws.datasets.keys():\n",
    "    print(\"Found existing dataset, use it.\")\n",
    "    found = True\n",
    "    dataset = ws.datasets[key] # already registered\n",
    "    \n",
    "if not found:\n",
    "    example_data = \"https://media.githubusercontent.com/media/satriawadhipurusa/ml-dataset-collection/master/Fraud-Detection/creditcard-fraud.csv\" # uploaded to Git for download\n",
    "    dataset = Dataset.Tabular.from_delimited_files(example_data)\n",
    "    dataset = dataset.register(workspace=ws, name=key, description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_pandas_dataframe().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gather": {
     "logged": 1598531923519
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "source": [
    "## Hyperdrive Configuration\n",
    "\n",
    "We will use Hyperdrive to search the best hyperparameters of the model. The model will be using `SupportVectorClassifier` or SVC, this model excel in separating hyperplanes of different classes, especially in finding anomaly (**fraud**). This is due the `class_weight` parameter of the model that can be set for imbalanced dataset. \n",
    "\n",
    "The followings are the hyperparameter of SVC:\n",
    "\n",
    "* gamma (Kernel Coefficient): 0.01 - 100\n",
    "* C (regularization): 0.01 - 100\n",
    "* class weight: `{0: 0.05, 1: 0.95}`, `{0: 0.1, 1: 0.9}`, `{0: 0.25, 1: 0.75}`\n",
    "\n",
    "These three parameters are essentials in SVC, and both the gamma and C use a very large parameter space (0.01 - 100). Since we also limited in time and budget, we will use `BayesianParameterSampling` to make the search more informed. Using this sampling method, the algorithm will learn from previous runs to narrow the search space on a parameter that will maximize the objective function, which is maximize the primary metric. Since it's using bayesian, `NoTeriminationPolicy` will be used instead.\n",
    "\n",
    "Finally, we set the primary metric name as **\"AUC Weighted\"** instead of Accuracy, it is suited for this type of imbalanced dataset. This metric also used in Automated ML previously so we can compare them on the same ground. The other config, we will maximize `max_total_runs` and `max_duration_minutes`, since bayesian sampling usually took a longer than randomized search or grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create an early termination policy. This is not required if you are using Bayesian sampling.\n",
    "early_termination_policy = NoTerminationPolicy()\n",
    "\n",
    "#TODO: Create the different params that you will be using during training.\n",
    "param_sampling = BayesianParameterSampling({\n",
    "    \"gamma\": uniform(0.01, 100),\n",
    "    \"C\": uniform(0.01, 100),\n",
    "    \"class_weight\": choice(\n",
    "        \"{0: 0.05, 1: 0.95}\",\n",
    "        \"{0: 0.1, 1: 0.9}\",\n",
    "        \"{0: 0.25, 1: 0.75}\")\n",
    "})\n",
    "\n",
    "#TODO: Create your estimator and hyperdrive config\n",
    "environment = Environment.from_conda_specification(name=\"sklearn-env\", file_path=\"conda.yaml\")\n",
    "arguments = [\n",
    "    \"--gamma\",\n",
    "    1.0,\n",
    "    \"--C\",\n",
    "    1.0,\n",
    "    \"--class_weight\",\n",
    "    \"{0: 0.05, 1: 0.95}\"\n",
    "]\n",
    "estimator = ScriptRunConfig(source_directory=\".\",\n",
    "                            script=\"./training/train.py\",\n",
    "                            arguments=arguments,\n",
    "                            environment=environment,\n",
    "                            compute_target=compute_target)\n",
    "\n",
    "hyperdrive_run_config = HyperDriveConfig(\n",
    "    hyperparameter_sampling=param_sampling,\n",
    "    primary_metric_name=\"AUC_Weighted\",\n",
    "    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "    run_config=estimator,\n",
    "    policy=early_termination_policy,\n",
    "    max_total_runs=60,\n",
    "    max_concurrent_runs=2,\n",
    "    max_duration_minutes=60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598544897941
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Submit your experiment\n",
    "remote_run = experiment.submit(hyperdrive_run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gather": {
     "logged": 1598544898497
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Run Details\n",
    "\n",
    "The different runs show that some run will have much higher metric than other runs. It's the bayesian sampling job to find which parameters can produce the best run with highest metrics. We see that the best metric is **0.919** in AUC Weighted with **0.275 gamma**, **44.808 C**, and `class_weight` of `{0: 0.25, 1: 0.75}`. This is smaller than Automated ML, and hence we can decide that we will not deploy this model but deploy the Automated ML one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598546648408
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "RunDetails(remote_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598546650307
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "best_hyperdrive_run = remote_run.get_best_run_by_primary_metric()\n",
    "\n",
    "print(f\"Best HyperDrive Run:\\n\\n{best_hyperdrive_run}\")\n",
    "print(\"==============\")\n",
    "namefile = \"outputs/model.joblib\"\n",
    "best_hyperdrive_run.download_file(namefile, namefile) # save the best model\n",
    "best_hyperdrive_model = joblib.load(open(namefile, \"rb\"))\n",
    "\n",
    "print(f\"Best HyperDrive Model:\\n\\n{best_hyperdrive_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Model Deployment\n",
    "\n",
    "Remember you have to deploy only one of the two models you trained but you still need to register both the models. Perform the steps in the rest of this notebook only if you wish to deploy this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the model\n",
    "best_hyperdrive_run.register_model(model_name=\"credit-fraud-model\", model_path=\"outputs/model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission Checklist**\n",
    "- I have registered the model.\n",
    "- I have deployed the model with the best accuracy as a webservice.\n",
    "- I have tested the webservice by sending a request to the model endpoint.\n",
    "- I have deleted the webservice and shutdown all the computes that I have used.\n",
    "- I have taken a screenshot showing the model endpoint as active.\n",
    "- The project includes a file containing the environment details.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
