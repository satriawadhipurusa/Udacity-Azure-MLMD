{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598423888013
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from azureml.core import Environment\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.webservice.aci import AciWebservice\n",
    "from azureml.core.model import InferenceConfig, Model\n",
    "\n",
    "from azureml.train.automl import AutoMLConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset\n",
    "\n",
    "### Overview\n",
    "In this experiment we will be using **Kaggle - Credit Card Fraud Dataset**, the dataset can be downloaded from [here](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud). This dataset consist of **~99% Non-Fraudulent** transactions while the rest **~0.1% Fraudulent** transaction, hence the data is imbalance. There are no null values, and all the columns except for the **transaction** and **amount** are unknown, maybe for privacy reasons. As additional note, all the data in this datasets has been scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598423890461
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config(\"./config.json\")\n",
    "\n",
    "# choose a name for experiment\n",
    "experiment_name = \"creditcard-experiment\"\n",
    "project_folder = './creditcard-pipeline-project'\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"creditcard-dataset\"\n",
    "description = \"Credit Card - Dealing from Imbalance Datasets from https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\"\n",
    "\n",
    "found = False\n",
    "if key in ws.datasets.keys():\n",
    "    print(\"Found existing dataset, use it.\")\n",
    "    found = True\n",
    "    dataset = ws.datasets[key] # already registered\n",
    "    \n",
    "if not found:\n",
    "    example_data = \"https://media.githubusercontent.com/media/satriawadhipurusa/ml-dataset-collection/master/Fraud-Detection/creditcard-fraud.csv\" # uploaded to Git for download\n",
    "    dataset = Dataset.Tabular.from_delimited_files(example_data)\n",
    "    dataset = dataset.register(workspace=ws, name=key, description=description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## AutoML Configuration\n",
    "\n",
    "The AutoML usually consist of the followings configurations:\n",
    "\n",
    "* Compute Target: For the compute target we will use `STANDARD_D2_v3` (2CPU, 8GB memory, 50GB storage) in low priority which has been created earlier, and max nodes of 4, this will enable more parallel trials in training Automated ML \n",
    "* Task: Since we're predicting Fraud (0/1), this should be binary **Classification** task\n",
    "* Early Stopping \n",
    "  * Timeout: We set the timeout to be 30 mins instead of 60 mins, so we can iterate faster\n",
    "  * Primary Metric: We are interested to see **AUC Weighted** with **0.98** exit score, since this is an imbalance dataset. An accuracy of 0.99 will be misleading since we can achieve the same accuracy with just predicting 0s, but low precision/recall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amlcompute_cluster_name = \"automl-cls\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
    "    print(\"Found existing cluster, use it.\")\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_D2_v3\", max_nodes=4)\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True, min_node_count=0, timeout_in_minutes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598429217746
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Put your automl settings here\n",
    "automl_settings = {\n",
    "    \"experiment_timeout_minutes\": 30,\n",
    "    \"max_concurrent_iterations\": 4,\n",
    "    \"primary_metric\": \"AUC_weighted\",\n",
    "    \"experiment_exit_score\" : 0.98\n",
    "}\n",
    "\n",
    "# TODO: Put your automl config here\n",
    "automl_config = AutoMLConfig(compute_target=compute_target,\n",
    "                             task=\"classification\",\n",
    "                             training_data=dataset,\n",
    "                             label_column_name=\"Class\",\n",
    "                             path=project_folder,\n",
    "                             enable_early_stopping=True,\n",
    "                             featurization=\"auto\",\n",
    "                             debug_log=\"automl_errors.log\",\n",
    "                             **automl_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431107951
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Submit your experiment\n",
    "remote_run = experiment.submit(automl_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Details\n",
    "\n",
    "There are couple of **models**, **preprocessor**, and **hyperparameters** trained in this Automated ML experiment, some of the models are:\n",
    "\n",
    "* LightGBM\n",
    "* RandomForest\n",
    "* XGBoost\n",
    "* ExtremeRandomTrees\n",
    "* LogisticRegression\n",
    "* VotingEnsemble\n",
    "* StackEnsemble\n",
    "\n",
    "These **worst** model is combination of **PCA** and **LighGBM** with AUC Weighted of **0.72**, while the best one is a combination of **StandardScalerWrapper** with **LightGBM** with AUC Weighted of **0.9739**.\n",
    "\n",
    "This discrepancy due to the nature of the data and the nature of the modeling done on those data. \n",
    "* Since this datasets already very condensed in information, a PCA to reduce the information may backfire and reduce the overall metric\n",
    "* Standard scaler to scale the various numerical values, can help the learning algorithm to classifify fraud / non-fraud better. \n",
    "\n",
    "The other combinations also stemmed from preprocessor that are not suited, algorithm or hyperparameters that may overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431121770
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(remote_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431425670
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "best_automl_run, best_automl_model = remote_run.get_output()\n",
    "print(f\"Best AutoML Run:\\n\\n{best_automl_run}\")\n",
    "print(\"==============\")\n",
    "print(f\"Best AutoML Model:\\n\\n{best_automl_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431426111
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Save the best model\n",
    "print(\"Saving the best Model.....\")\n",
    "model_path = \"outputs/model.pkl\"\n",
    "pickle.dump(best_automl_model, open(model_path, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "Remember you have to deploy only one of the two models you trained but you still need to register both the models. Perform the steps in the rest of this notebook only if you wish to deploy this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431435189
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Register the model\n",
    "best_automl_run.register_model(model_name=\"credit-fraud-model\", model_path=\"outputs/model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598432707604
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "best_automl_run.download_file(\"outputs/conda_env_v_1_0_0.yml\", \"conda.yaml\")\n",
    "env = Environment.from_conda_specification(name=\"env\", file_path=\"conda.yaml\")\n",
    "\n",
    "best_automl_run.download_file(\"outputs/scoring_file_v_2_0_0.py\", \"score.py\")\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\", environment=env)\n",
    "\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1, auth_enabled=True)\n",
    "\n",
    "model = Model(ws, \"credit-fraud-model\", version=1, run_id=best_automl_run.id)\n",
    "webservice = Model.deploy(ws, \"credit-fraud-model\",\n",
    "                          models=[model],\n",
    "                          inference_config=inference_config,\n",
    "                          deployment_config=deployment_config,\n",
    "                          overwrite=True)\n",
    "webservice.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Scoring URI:\\n\\n{webservice.scoring_uri}\")\n",
    "print(\"==============\")\n",
    "primary_key, secondary_key = webservice.get_keys()\n",
    "print(f\"Primary Key:\\n\\n{primary_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.to_pandas_dataframe()\n",
    "sample = df.drop(columns=\"Class\").sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_json = sample.to_dict(orient=\"record\")\n",
    "data = {\n",
    "    \"Inputs\": {\n",
    "        \"data\": sample_json\n",
    "    },\n",
    "    \"GlobalParameters\": {\n",
    "        \"method\": \"predict\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Sample Data:\\n\\n:{json.dumps(data, indent=2)}\")\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {primary_key}\"\n",
    "}\n",
    "response = requests.post(webservice.scoring_uri, json=data, headers=headers)\n",
    "print(\"=========\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "webservice.update(enable_app_insights=True)  # enable app insights\n",
    "logs = webservice.get_logs()\n",
    "for line in logs.split('\\n'):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission Checklist**\n",
    "- I have registered the model.\n",
    "- I have deployed the model with the best accuracy as a webservice.\n",
    "- I have tested the webservice by sending a request to the model endpoint.\n",
    "- I have deleted the webservice and shutdown all the computes that I have used.\n",
    "- I have taken a screenshot showing the model endpoint as active.\n",
    "- The project includes a file containing the environment details.\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python [conda env:udacity-az-mlnd]",
   "language": "python",
   "name": "conda-env-udacity-az-mlnd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
